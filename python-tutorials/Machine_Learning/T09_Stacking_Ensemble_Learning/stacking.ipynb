{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Ensemble Learning\n",
    "\n",
    "## Problem Type\n",
    "**Stacking Ensemble Learning** is primarily used for:\n",
    "- **Supervised Learning**\n",
    "- **Regression** and **Classification** tasks\n",
    "- **Applications**: Any predictive modeling problem where multiple base models can be combined to improve performance (e.g., tabular data, image classification, NLP tasks).\n",
    "\n",
    "### How Stacking Ensemble Learning Works\n",
    "- **Base Learners:**\n",
    "  - Stacking uses multiple diverse models (e.g., decision trees, SVMs, neural networks) as base learners, each trained on the same dataset.\n",
    "- **Meta-Learner:**\n",
    "  - A meta-learner (often a simple model like linear regression or logistic regression) is trained on the predictions made by the base learners, combining them into a final prediction.\n",
    "- **Layering:**\n",
    "  - The process involves layering the models where the first layer consists of base learners and the final layer consists of the meta-learner.\n",
    "- **Training:**\n",
    "  - Each base learner is trained on the original training data, and their predictions are used as input features to train the meta-learner.\n",
    "- **Cross-Validation:**\n",
    "  - Often, cross-validation is used to generate predictions from the base learners to ensure that the meta-learner is not overfitting to the base learners' predictions.\n",
    "- **Model Diversity:**\n",
    "  - The key to stacking is using a diverse set of base learners that capture different aspects of the data, improving the ensemble's overall performance.\n",
    "- **Prediction:**\n",
    "  - During inference, the base learners make predictions on the input data, and the meta-learner combines these predictions to produce the final output.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`base_learners`:**\n",
    "  - **Description:** Types and number of base models used in the stacking ensemble.\n",
    "  - **Impact:** Diversity among base learners helps in capturing different data patterns, improving ensemble performance.\n",
    "  - **Default:** Common choices include decision trees, SVMs, and neural networks.\n",
    "- **`meta_learner`:**\n",
    "  - **Description:** The model used to aggregate predictions from base learners.\n",
    "  - **Impact:** A simple, robust meta-learner (e.g., logistic regression for classification) can effectively combine diverse base models' predictions.\n",
    "  - **Default:** Often a linear model like logistic regression or linear regression.\n",
    "- **`cross_validation_splits`:**\n",
    "  - **Description:** Number of folds in cross-validation used for generating base learners' predictions.\n",
    "  - **Impact:** More splits can reduce overfitting but increase computational cost.\n",
    "  - **Default:** Typically `5` or `10` folds.\n",
    "  \n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| Can significantly improve model performance by combining strengths of diverse models | Computationally expensive, especially with many base learners |\n",
    "| Reduces the risk of overfitting compared to individual models | Requires careful selection and tuning of base learners and meta-learner |\n",
    "| Versatile and can be applied to both regression and classification tasks | Difficult to interpret and understand the final model due to multiple layers |\n",
    "| Handles both linear and non-linear relationships effectively | More complex to implement and tune compared to simpler ensemble methods |\n",
    "| Robust to noisy data if base learners are diverse     | High risk of overfitting if cross-validation or stacking is not properly configured |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy (Classification):**\n",
    "  - **Description:** Ratio of correct predictions to total predictions.\n",
    "  - **Good Value:** Higher is better; values above 0.85 indicate strong model performance.\n",
    "  - **Bad Value:** Below 0.5 suggests poor model performance.\n",
    "- **Precision (Classification):**\n",
    "  - **Description:** Proportion of true positives among all positive predictions.\n",
    "  - **Good Value:** Higher values indicate fewer false positives, especially important in imbalanced datasets.\n",
    "  - **Bad Value:** Low values suggest many false positives.\n",
    "- **Recall (Classification):**\n",
    "  - **Description:** Proportion of actual positives correctly identified.\n",
    "  - **Good Value:** Higher values indicate fewer false negatives, important in recall-sensitive applications.\n",
    "  - **Bad Value:** Low values suggest many false negatives.\n",
    "- **F1 Score (Classification):**\n",
    "  - **Description:** Harmonic mean of Precision and Recall.\n",
    "  - **Good Value:** Higher values indicate a good balance between Precision and Recall.\n",
    "  - **Bad Value:** Low values suggest a poor balance between Precision and Recall.\n",
    "- **R-squared (Regression):**\n",
    "  - **Description:** Proportion of variance in the dependent variable explained by the model.\n",
    "  - **Good Value:** Higher is better; values closer to 1 indicate a strong model.\n",
    "  - **Bad Value:** Values closer to 0 suggest the model does not explain much of the variance.\n",
    "- **Mean Absolute Error (MAE) (Regression):**\n",
    "  - **Description:** Measures the average absolute difference between predicted and actual values.\n",
    "  - **Good Value:** Lower is better; values close to `0` indicate high accuracy.\n",
    "  - **Bad Value:** Higher values suggest significant prediction errors.\n",
    "- **Root Mean Squared Error (RMSE) (Regression):**\n",
    "  - **Description:** Measures the square root of the average squared difference between predicted and actual values.\n",
    "  - **Good Value:** Lower is better; values close to `0` indicate high accuracy.\n",
    "  - **Bad Value:** Higher values suggest the model's predictions deviate significantly from actual values.\n",
    "- **AUC-ROC (Classification):**\n",
    "  - **Description:** Measures the model's ability to distinguish between classes across all thresholds.\n",
    "  - **Good Value:** Values closer to 1 indicate strong separability between classes.\n",
    "  - **Bad Value:** Values near 0.5 suggest random guessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Define the base models\n",
    "base_models = [\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    (\"svc\", SVC()),\n",
    "]\n",
    "\n",
    "# Define the meta model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Define the stacking classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = stacking_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the stacking model\n",
    "predictions = stacking_model.predict(X_test)\n",
    "print(\"Stacking Model Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Evaluate each base model\n",
    "for name, model in stacking_model.named_estimators_.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(f\"{name} Accuracy: \", accuracy_score(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
