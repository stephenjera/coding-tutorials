{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks (FNN)\n",
    "\n",
    "## Problem Type\n",
    "**Feedforward Neural Networks (FNN)** are primarily used for:\n",
    "- **Supervised Learning**\n",
    "- **Regression** and **Classification** tasks\n",
    "- **Applications**: Image recognition, speech recognition, tabular data analysis, and many other predictive modeling tasks.\n",
    "\n",
    "### How Feedforward Neural Networks Work\n",
    "- **Input Layer:**\n",
    "  - The input layer receives the data. Each neuron in this layer represents a feature in the dataset.\n",
    "- **Hidden Layers:**\n",
    "  - FNNs consist of one or more hidden layers where computations are performed. Each neuron in these layers applies a linear transformation followed by a non-linear activation function (e.g., ReLU, sigmoid).\n",
    "- **Activation Functions:**\n",
    "  - Non-linear activation functions allow the network to model complex relationships. Common functions include ReLU, sigmoid, and tanh.\n",
    "- **Output Layer:**\n",
    "  - The final layer outputs predictions. For classification, it might use a softmax function to output probabilities. For regression, it outputs a continuous value.\n",
    "- **Forward Propagation:**\n",
    "  - Input data is passed through the network, layer by layer, in a forward direction, to generate an output.\n",
    "- **Loss Function:**\n",
    "  - The difference between the predicted output and the actual target is measured using a loss function (e.g., cross-entropy for classification, mean squared error for regression).\n",
    "- **Backpropagation:**\n",
    "  - The network uses backpropagation to compute the gradient of the loss function with respect to each weight by applying the chain rule, allowing the network to update weights via gradient descent.\n",
    "- **Weight Updating:**\n",
    "  - The network's weights are adjusted to minimize the loss function. This process is repeated over many iterations (epochs) until the model converges or meets a predefined stopping criterion.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`number_of_layers`:**\n",
    "  - **Description:** Number of hidden layers in the network.\n",
    "  - **Impact:** More layers can capture complex patterns but may lead to overfitting if the network is too deep.\n",
    "  - **Default:** Typically ranges from `1` to `3`, but deeper networks are used in more complex tasks.\n",
    "- **`number_of_neurons_per_layer`:**\n",
    "  - **Description:** Number of neurons in each hidden layer.\n",
    "  - **Impact:** More neurons allow for capturing more features but increase the risk of overfitting and require more computational resources.\n",
    "  - **Default:** Common choices range from `64` to `512` per layer.\n",
    "- **`activation_function`:**\n",
    "  - **Description:** Function used to introduce non-linearity into the model.\n",
    "  - **Impact:** ReLU is commonly used due to its performance benefits, but other functions like sigmoid or tanh may be used depending on the task.\n",
    "  - **Default:** `ReLU` for hidden layers, `softmax` for classification output, `linear` for regression output.\n",
    "- **`learning_rate`:**\n",
    "  - **Description:** Step size for updating the network's weights during training.\n",
    "  - **Impact:** Higher values speed up training but may cause instability; lower values provide more stable convergence but slow down training.\n",
    "  - **Default:** Ranges from `1e-3` to `1e-5`, often adjusted dynamically using a learning rate scheduler.\n",
    "- **`batch_size`:**\n",
    "  - **Description:** Number of samples processed before the model's weights are updated.\n",
    "  - **Impact:** Larger batch sizes lead to more stable gradient estimates but require more memory.\n",
    "  - **Default:** Typically `32`, `64`, or `128`.\n",
    "- **`epochs`:**\n",
    "  - **Description:** Number of times the entire dataset is passed through the network during training.\n",
    "  - **Impact:** More epochs allow the model to learn better but may lead to overfitting if trained too long.\n",
    "  - **Default:** Usually ranges from `10` to `100`, depending on the dataset and task.\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| Capable of modeling complex, non-linear relationships | Requires large amounts of data and computational power |\n",
    "| Flexible architecture that can be adapted to various types of data | Prone to overfitting, especially with deep architectures |\n",
    "| Can approximate any continuous function given sufficient neurons and layers (Universal Approximation Theorem) | Often requires extensive hyperparameter tuning to achieve optimal performance |\n",
    "| Benefits from modern hardware acceleration (e.g., GPUs) | Black-box nature makes it difficult to interpret the model |\n",
    "| Extensive support in deep learning frameworks (e.g., TensorFlow, PyTorch) | Training can be time-consuming and resource-intensive |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy (Classification):**\n",
    "  - **Description:** Ratio of correct predictions to total predictions.\n",
    "  - **Good Value:** Higher is better; values above 0.85 indicate strong model performance.\n",
    "  - **Bad Value:** Below 0.5 suggests poor model performance.\n",
    "- **Precision (Classification):**\n",
    "  - **Description:** Proportion of true positives among all positive predictions.\n",
    "  - **Good Value:** Higher values indicate fewer false positives, especially important in imbalanced datasets.\n",
    "  - **Bad Value:** Low values suggest many false positives.\n",
    "- **Recall (Classification):**\n",
    "  - **Description:** Proportion of actual positives correctly identified.\n",
    "  - **Good Value:** Higher values indicate fewer false negatives, important in recall-sensitive applications.\n",
    "  - **Bad Value:** Low values suggest many false negatives.\n",
    "- **F1 Score (Classification):**\n",
    "  - **Description:** Harmonic mean of Precision and Recall.\n",
    "  - **Good Value:** Higher values indicate a good balance between Precision and Recall.\n",
    "  - **Bad Value:** Low values suggest a poor balance between Precision and Recall.\n",
    "- **R-squared (Regression):**\n",
    "  - **Description:** Proportion of variance in the dependent variable explained by the model.\n",
    "  - **Good Value:** Higher is better; values closer to 1 indicate a strong model.\n",
    "  - **Bad Value:** Values closer to 0 suggest the model does not explain much of the variance.\n",
    "- **Mean Absolute Error (MAE) (Regression):**\n",
    "  - **Description:** Measures the average absolute difference between predicted and actual values.\n",
    "  - **Good Value:** Lower is better; values close to `0` indicate high accuracy.\n",
    "  - **Bad Value:** Higher values suggest significant prediction errors.\n",
    "- **Root Mean Squared Error (RMSE) (Regression):**\n",
    "  - **Description:** Measures the square root of the average squared difference between predicted and actual values.\n",
    "  - **Good Value:** Lower is better; values close to `0` indicate high accuracy.\n",
    "  - **Bad Value:** Higher values suggest the model's predictions deviate significantly from actual values.\n",
    "- **Log Loss (Classification):**\n",
    "  - **Description:** Measures the performance of a classification model where the output is a probability value between 0 and 1.\n",
    "  - **Good Value:** Lower is better; values close to `0` indicate that the model's predictions are close to the true labels.\n",
    "  - **Bad Value:** Higher values suggest poor prediction probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = torch.tensor(iris.data, dtype=torch.float32).to(device)\n",
    "y = torch.tensor(iris.target, dtype=torch.long).to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(IrisClassifier, self).__init__()\n",
    "    self.fc1 = nn.Linear(4, 10)  # Input layer to first hidden layer\n",
    "    self.relu = nn.ReLU()  # Activation function\n",
    "    self.fc2 = nn.Linear(10, 10)  # First hidden layer to second hidden layer\n",
    "    self.output = nn.Linear(10, 3)  # Second hidden layer to output layer\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.relu(self.fc2(x))\n",
    "    x = torch.nn.functional.softmax(self.output(x), dim=1)  # Apply softmax for probability distribution\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model instance and move it to the device\n",
    "model = IrisClassifier().to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "  # Forward pass\n",
    "  y_pred = model(X_train)\n",
    "  loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "  # Backward pass and parameter update\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  # Print training progress\n",
    "  if epoch % 10 == 0:\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data (already on device)\n",
    "with torch.no_grad():\n",
    "  y_pred = model(X_test)\n",
    "  _, predicted = torch.max(y_pred, 1)\n",
    "  accuracy = (predicted == y_test).sum().float() / len(y_test)\n",
    "  print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
