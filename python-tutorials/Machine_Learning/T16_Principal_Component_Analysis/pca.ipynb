{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "## Problem Type\n",
    "**Principal Component Analysis (PCA)** is primarily used for:\n",
    "- **Dimensionality Reduction** in both:\n",
    "  - **Supervised** learning (as a preprocessing step)\n",
    "  - **Unsupervised** learning (for exploratory data analysis)\n",
    "- **Feature Extraction** and **Visualization**\n",
    "\n",
    "### How PCA Works\n",
    "- **Linear transformation:**\n",
    "  - Transforms the original correlated features into a smaller set of uncorrelated features called principal components.\n",
    "- **Maximizes variance:**\n",
    "  - Each principal component captures the maximum variance from the original dataset.\n",
    "  - The first principal component captures the highest variance, the second captures the next highest, and so on.\n",
    "- **Orthogonal components:**\n",
    "  - Principal components are orthogonal to each other, ensuring no redundancy in the new feature set.\n",
    "- **Eigenvectors and Eigenvalues:**\n",
    "  - PCA identifies the eigenvectors (directions) and eigenvalues (magnitude) of the covariance matrix of the data.\n",
    "  - Eigenvectors form the principal components, and eigenvalues determine the amount of variance captured by each component.\n",
    "- **Dimensionality reduction:**\n",
    "  - By selecting the top `k` principal components, PCA reduces the dimensionality of the dataset while retaining most of the variance.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`n_components`:**\n",
    "  - **Description:** Number of principal components to keep.\n",
    "  - **Impact:** Reducing `n_components` reduces the datasetâ€™s dimensionality but may lose some variance.\n",
    "  - **Default:** `None` (all components are kept).\n",
    "- **`svd_solver`:**\n",
    "  - **Description:** Algorithm used to compute the principal components (`auto`, `full`, `arpack`, `randomized`).\n",
    "  - **Impact:** Determines the computation method for PCA. `randomized` is efficient for large datasets.\n",
    "  - **Default:** `auto`.\n",
    "- **`whiten`:**\n",
    "  - **Description:** If `True`, the components are scaled by their respective eigenvalues.\n",
    "  - **Impact:** Removes the unit variance from each principal component, which is useful for some downstream tasks.\n",
    "  - **Default:** `False`.\n",
    "- **`tol`:**\n",
    "  - **Description:** Tolerance for stopping criterion.\n",
    "  - **Impact:** Controls the precision of the solver, particularly for iterative solvers like `arpack`.\n",
    "  - **Default:** `0.0`.\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| Reduces dimensionality, simplifying models            | May lose interpretability by transforming features into principal components |\n",
    "| Captures the most variance in the data                | Assumes linearity, which might not capture complex relationships |\n",
    "| Helps mitigate multicollinearity in features          | Sensitive to the scaling of features, requiring normalization |\n",
    "| Improves computational efficiency for large datasets  | May discard smaller, but potentially important, variance when reducing dimensions |\n",
    "| Can be used as a preprocessing step before modeling   | Not ideal for non-Gaussian distributions or non-linear relationships |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Explained Variance Ratio:**\n",
    "  - **Description:** Proportion of variance captured by each principal component.\n",
    "  - **Good Value:** Values close to 1 for the first few components indicate effective dimensionality reduction.\n",
    "  - **Bad Value:** Low values across all components suggest ineffective capture of variance.\n",
    "- **Cumulative Explained Variance:**\n",
    "  - **Description:** Cumulative sum of the explained variance ratios.\n",
    "  - **Good Value:** Typically, 95% cumulative explained variance is considered sufficient.\n",
    "  - **Bad Value:** If a large number of components are needed to reach 95%, PCA might not be reducing dimensionality effectively.\n",
    "- **Reconstruction Error:**\n",
    "  - **Description:** Difference between the original data and the data reconstructed from the selected principal components.\n",
    "  - **Good Value:** Lower values indicate better retention of original data information.\n",
    "  - **Bad Value:** High values suggest significant information loss due to dimensionality reduction.\n",
    "- **Scree Plot:**\n",
    "  - **Description:** Visual representation of eigenvalues, showing the amount of variance explained by each component.\n",
    "  - **Good Value:** A sharp drop followed by a plateau suggests that only the first few components are needed.\n",
    "  - **Bad Value:** A gradual decline suggests that more components are required to capture variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Labels\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=4, svd_solver=\"auto\", whiten=False, tol=0)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained Variance Ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Print explained variance ratios\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print(\"Cumulative Explained Variance:\", cumulative_explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Scree Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    np.arange(1, len(explained_variance_ratio) + 1),\n",
    "    explained_variance_ratio,\n",
    "    \"o-\",\n",
    "    label=\"Explained Variance Ratio\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(1, len(cumulative_explained_variance) + 1),\n",
    "    cumulative_explained_variance,\n",
    "    \"o-\",\n",
    "    label=\"Cumulative Explained Variance\",\n",
    ")\n",
    "plt.axhline(y=0.95, color=\"r\", linestyle=\"--\", label=\"95% Threshold\")\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.xlabel(\"Principal Components\")\n",
    "plt.ylabel(\"Variance Explained\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
