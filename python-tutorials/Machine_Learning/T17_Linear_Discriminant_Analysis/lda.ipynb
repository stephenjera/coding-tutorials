{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)\n",
    "\n",
    "## Problem Type\n",
    "**Linear Discriminant Analysis (LDA)** is primarily used for:\n",
    "- **Classification** problems\n",
    "- **Supervised** learning\n",
    "\n",
    "### How LDA Works\n",
    "- **Dimensionality reduction and classification:**\n",
    "  - Projects data onto a lower-dimensional space while maintaining the class-discriminatory information.\n",
    "- **Maximizes class separability:**\n",
    "  - Finds a linear combination of features that best separates two or more classes.\n",
    "  - Maximizes the ratio of between-class variance to within-class variance.\n",
    "- **Assumes normal distribution:**\n",
    "  - Assumes that each class follows a Gaussian distribution with a different mean but the same covariance matrix.\n",
    "- **Decision boundary:**\n",
    "  - Computes linear decision boundaries between classes in the transformed space.\n",
    "- **Training process:**\n",
    "  - Involves estimating class means, shared covariance, and priors, and then using these to define the linear discriminants.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`solver`:**\n",
    "  - **Description:** Algorithm to use for the computation (`svd`, `lsqr`, `eigen`).\n",
    "  - **Impact:** `svd` does not compute the covariance matrix and is more stable, while `lsqr` and `eigen` are suitable for larger datasets with more predictors.\n",
    "  - **Default:** `svd`.\n",
    "- **`shrinkage`:**\n",
    "  - **Description:** A regularization technique to adjust the covariance estimate; useful when `solver` is `lsqr` or `eigen`.\n",
    "  - **Impact:** Helps prevent overfitting, especially with high-dimensional data.\n",
    "  - **Default:** `None`.\n",
    "- **`n_components`:**\n",
    "  - **Description:** Number of linear discriminants to compute.\n",
    "  - **Impact:** Reduces the dimensionality of the feature space; typically `n_classes - 1`.\n",
    "  - **Default:** `None` (automatically set based on the number of classes).\n",
    "- **`priors`:**\n",
    "  - **Description:** Class priors used in the model; can be uniform or specified based on prior knowledge.\n",
    "  - **Impact:** Adjusts the model's bias toward certain classes based on prior probabilities.\n",
    "  - **Default:** `None` (automatically inferred from the training data).\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| Effective for small sample sizes with normally distributed data | Assumes linear decision boundaries, which may not capture complex relationships |\n",
    "| Reduces dimensionality while retaining class information | Sensitive to the assumption of equal covariance matrices across classes |\n",
    "| Computationally efficient and interpretable           | Performance degrades if classes have similar means     |\n",
    "| Robust to overfitting, especially with regularization | May struggle with non-Gaussian distributions and outliers |\n",
    "| Works well with linearly separable classes            | Less effective with a large number of features relative to observations |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy (Classification):**\n",
    "  - **Description:** Ratio of correct predictions to total predictions.\n",
    "  - **Good Value:** Higher is better; generally, values above 0.85 indicate strong model performance.\n",
    "  - **Bad Value:** Below 0.5 suggests poor model performance.\n",
    "- **Precision (Classification):**\n",
    "  - **Description:** Proportion of positive identifications that were actually correct.\n",
    "  - **Good Value:** Higher values indicate fewer false positives; crucial in imbalanced datasets.\n",
    "  - **Bad Value:** Low values suggest many false positives.\n",
    "- **Recall (Classification):**\n",
    "  - **Description:** Proportion of actual positives that were correctly identified.\n",
    "  - **Good Value:** Higher values indicate fewer false negatives; important in recall-sensitive applications.\n",
    "  - **Bad Value:** Low values suggest many false negatives.\n",
    "- **F1 Score (Classification):**\n",
    "  - **Description:** Harmonic mean of Precision and Recall.\n",
    "  - **Good Value:** Higher values indicate a good balance between Precision and Recall.\n",
    "  - **Bad Value:** Low values suggest a poor balance between Precision and Recall.\n",
    "- **AUC-ROC (Classification):**\n",
    "  - **Description:** Measures the ability of the model to distinguish between classes across all thresholds.\n",
    "  - **Good Value:** Values closer to 1 indicate strong separability between classes.\n",
    "  - **Bad Value:** Values near 0.5 suggest random guessing.\n",
    "- **Log Loss (Classification):**\n",
    "  - **Description:** Measures the performance of a classification model where the output is a probability value between 0 and 1.\n",
    "  - **Good Value:** Lower values indicate better model calibration and performance.\n",
    "  - **Bad Value:** Higher values suggest poor probabilistic predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LDA model with specified parameters\n",
    "lda = LinearDiscriminantAnalysis(\n",
    "    solver=\"svd\",  # Using SVD solver\n",
    "    shrinkage=None,  # No shrinkage applied (only used with 'lsqr' or 'eigen')\n",
    "    n_components=None,  # Automatically determine based on number of classes\n",
    "    priors=None,  # Use class priors inferred from the data\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = lda.predict_proba(X_test)\n",
    "\n",
    "# Predict class labels\n",
    "y_pred = lda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using log loss\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "print(f\"Log Loss: {logloss:.2f}\")\n",
    "\n",
    "# Print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
