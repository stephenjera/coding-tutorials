{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "## Problem Type\n",
    "**Decision Trees** are primarily used for:\n",
    "- **Classification** problems\n",
    "- **Regression** problems\n",
    "- **Supervised** learning\n",
    "\n",
    "### How Decision Trees Work\n",
    "- **Tree-like model of decisions:** The algorithm splits the dataset into subsets based on the value of input features, creating branches for each possible outcome.\n",
    "- **Recursive binary splitting:** Continues until the algorithm either perfectly classifies the data or reaches a stopping criterion.\n",
    "- **Nodes and leaves:**\n",
    "  - **Nodes:** Represent a decision based on a feature.\n",
    "  - **Leaves:** Represent the outcome (class label in classification, value in regression).\n",
    "- **Splitting criteria:** Uses metrics like Gini impurity, Information Gain (Entropy), or Mean Squared Error (MSE) to determine the best split.\n",
    "- **Greedy algorithm:** Typically, Decision Trees use a greedy approach to make locally optimal decisions at each node.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`max_depth`:**\n",
    "  - **Description:** The maximum depth of the tree.\n",
    "  - **Impact:** Limits how deep the tree can grow; deeper trees can model more complex relationships but are prone to overfitting.\n",
    "  - **Default:** No limit (`None`), meaning nodes are expanded until all leaves are pure or contain less than `min_samples_split` samples.\n",
    "- **`min_samples_split`:**\n",
    "  - **Description:** The minimum number of samples required to split an internal node.\n",
    "  - **Impact:** Higher values prevent the model from learning overly specific patterns (reduces overfitting).\n",
    "  - **Default:** `2`.\n",
    "- **`min_samples_leaf`:**\n",
    "  - **Description:** The minimum number of samples required to be at a leaf node.\n",
    "  - **Impact:** Helps in smoothing the model, especially in regression trees; higher values make the model more resistant to noise.\n",
    "  - **Default:** `1`.\n",
    "- **`max_features`:**\n",
    "  - **Description:** The number of features to consider when looking for the best split.\n",
    "  - **Impact:** Reduces overfitting and variance when randomly selected; `sqrt(n_features)` is a good starting point for classification.\n",
    "  - **Default:** `None` (consider all features).\n",
    "- **`criterion`:**\n",
    "  - **Description:** The function to measure the quality of a split (`gini` for classification, `entropy` for classification, `mse` for regression).\n",
    "  - **Impact:** Affects how the tree makes splits; `gini` and `entropy` often yield similar trees, but their theoretical underpinnings differ.\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| Simple to understand and visualize                    | Prone to overfitting, especially with deep trees       |\n",
    "| No need for feature scaling                           | Can create biased trees if some classes dominate       |\n",
    "| Can handle both numerical and categorical data        | Sensitive to noisy data                                |\n",
    "| Requires little data preprocessing                    | Greedy algorithm may not find the globally optimal tree|\n",
    "| Can capture non-linear relationships                  | Can be unstable; small changes in data can lead to a completely different tree |\n",
    "| Fast and efficient to train                           | Less effective for very complex relationships compared to ensemble methods     |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy (Classification):**\n",
    "  - **Description:** Ratio of correct predictions to total predictions.\n",
    "  - **Good Value:** Higher is better; above 0.8 generally indicates a well-performing model.\n",
    "  - **Bad Value:** Below 0.5 suggests poor model performance, especially for balanced datasets.\n",
    "- **Precision (Classification):**\n",
    "  - **Description:** Proportion of positive identifications that were actually correct (True Positives / (True Positives + False Positives)).\n",
    "  - **Good Value:** Higher values indicate fewer false positives; important when the cost of a false positive is high.\n",
    "  - **Bad Value:** Low values suggest the model makes many false positives.\n",
    "- **Recall (Classification):**\n",
    "  - **Description:** Proportion of actual positives that were correctly identified (True Positives / (True Positives + False Negatives)).\n",
    "  - **Good Value:** Higher values indicate fewer false negatives; crucial when missing a positive case is costly.\n",
    "  - **Bad Value:** Low values suggest many false negatives.\n",
    "- **F1 Score (Classification):**\n",
    "  - **Description:** Harmonic mean of Precision and Recall; useful when you need a balance between the two.\n",
    "  - **Good Value:** Higher values are better; values above 0.7 indicate good performance.\n",
    "  - **Bad Value:** Lower values suggest an imbalance between precision and recall.\n",
    "- **Mean Squared Error (MSE) (Regression):**\n",
    "  - **Description:** Average of the squared differences between predicted and actual values.\n",
    "  - **Good Value:** Lower values indicate better fit; values close to 0 suggest minimal error.\n",
    "  - **Bad Value:** Higher values suggest a poor fit with significant prediction errors.\n",
    "- **R-squared (RÂ²) (Regression):**\n",
    "  - **Description:** Proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "  - **Good Value:** Closer to 1 indicates a good fit, but beware of overfitting if the value is too close to 1.\n",
    "  - **Bad Value:** Close to 0 suggests the model does not explain much of the variance.\n",
    "- **Cross-Validation Score:**\n",
    "  - **Description:** Measures the model's ability to generalize to an independent dataset.\n",
    "  - **Good Value:** Higher scores across folds indicate robust generalization.\n",
    "  - **Bad Value:** Low or inconsistent scores across folds suggest poor generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine.data  # Features\n",
    "y = wine.target  # Target labels (wine type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    max_features=None,\n",
    "    criterion='gini'\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=wine.target_names)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "\n",
    "# Print feature importances (optional)\n",
    "for feature, importance in zip(wine.feature_names, importances):\n",
    "    print(f\"{feature}: {importance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(model);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
