{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost (Adaptive Boosting)\n",
    "\n",
    "## Problem Type\n",
    "**AdaBoost (Adaptive Boosting)** is primarily used for:\n",
    "- **Classification** problems\n",
    "- **Regression** problems (though less common, with variants like AdaBoost.R2)\n",
    "- **Supervised** learning\n",
    "\n",
    "### How AdaBoost Works\n",
    "- **Boosting technique:**\n",
    "  - Sequentially combines multiple weak learners (typically decision stumps) to form a strong learner.\n",
    "  - Focuses on instances that were incorrectly predicted by previous models.\n",
    "- **Weighted samples:**\n",
    "  - In each iteration, AdaBoost adjusts the weights of misclassified instances, making them more influential in subsequent models.\n",
    "  - Correctly classified instances receive reduced weights, diminishing their impact on the next model.\n",
    "- **Model combination:**\n",
    "  - The final model is a weighted sum of all weak learners, with more accurate learners having higher weights.\n",
    "  - Reduces both bias and variance, leading to improved generalization.\n",
    "- **Iterative process:**\n",
    "  - Continues adding weak learners until a specified number is reached or no further improvements can be made.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`n_estimators`:**\n",
    "  - **Description:** Number of weak learners (e.g., decision stumps) to be added.\n",
    "  - **Impact:** More estimators generally improve performance but can lead to overfitting if too high.\n",
    "  - **Default:** `50`.\n",
    "- **`learning_rate`:**\n",
    "  - **Description:** Controls the contribution of each weak learner.\n",
    "  - **Impact:** Smaller values make the model more robust and require more estimators; typically in the range of `0.01` to `1`.\n",
    "  - **Default:** `1.0`.\n",
    "- **`estimator`:**\n",
    "  - **Description:** The weak learner algorithm used for boosting (e.g., decision tree with max depth = 1).\n",
    "  - **Impact:** Changing the base estimator can affect the modelâ€™s flexibility and performance.\n",
    "  - **Default:** `DecisionTreeClassifier(max_depth=1)`.\n",
    "- **`algorithm`:**\n",
    "  - **Description:** Specifies the boosting algorithm type (`SAMME` for multiclass classification or `SAMME.R` which uses probabilities from weak learners).\n",
    "  - **Impact:** `SAMME` typically performs better but requires probabilistic outputs from the base estimator.\n",
    "  - **Default:** `SAMME`.\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| Increases accuracy by combining weak learners         | Sensitive to noisy data and outliers                   |\n",
    "| Simple and interpretable when using decision stumps   | May require many estimators to achieve high accuracy   |\n",
    "| Effective on a wide range of classification tasks     | Can overfit if `n_estimators` is too large             |\n",
    "| Can be used with different types of weak learners     | Performance can degrade with weak learners that are too complex |\n",
    "| Does not require scaling of input data                | Slower to train and predict compared to simpler models |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy (Classification):**\n",
    "  - **Description:** Ratio of correct predictions to total predictions.\n",
    "  - **Good Value:** Higher is better; generally, values above 0.85 indicate good performance.\n",
    "  - **Bad Value:** Below 0.5 suggests poor model performance.\n",
    "- **Precision (Classification):**\n",
    "  - **Description:** Proportion of positive identifications that were actually correct.\n",
    "  - **Good Value:** Higher values indicate fewer false positives; crucial in imbalanced datasets.\n",
    "  - **Bad Value:** Low values suggest many false positives.\n",
    "- **Recall (Classification):**\n",
    "  - **Description:** Proportion of actual positives that were correctly identified.\n",
    "  - **Good Value:** Higher values indicate fewer false negatives; important in recall-sensitive applications.\n",
    "  - **Bad Value:** Low values suggest many false negatives.\n",
    "- **F1 Score (Classification):**\n",
    "  - **Description:** Harmonic mean of Precision and Recall.\n",
    "  - **Good Value:** Higher values indicate a good balance between Precision and Recall.\n",
    "  - **Bad Value:** Low values suggest an imbalance, with either high false positives or false negatives.\n",
    "- **Log Loss (Classification):**\n",
    "  - **Description:** Measures the performance of a classification model where the output is a probability value between 0 and 1.\n",
    "  - **Good Value:** Lower values indicate better model calibration and performance.\n",
    "  - **Bad Value:** Higher values suggest poor probabilistic predictions.\n",
    "- **AUC-ROC (Classification):**\n",
    "  - **Description:** Measures the ability of the model to distinguish between classes, summarizing performance across all classification thresholds.\n",
    "  - **Good Value:** Values closer to 1 indicate excellent separability.\n",
    "  - **Bad Value:** Values closer to 0.5 suggest random guessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report, log_loss, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the features (optional but often helpful)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AdaBoostClassifier with the specified parameters\n",
    "adaboost = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    algorithm='SAMME',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = adaboost.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Predict class labels\n",
    "y_pred = adaboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using log loss\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "print(f'Log Loss: {logloss:.2f}')\n",
    "\n",
    "# Evaluate the model using ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'ROC-AUC Score: {roc_auc:.2f}')\n",
    "\n",
    "# Print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot the ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
