{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM)\n",
    "\n",
    "## Problem Type\n",
    "**Long Short-Term Memory (LSTM)** networks are primarily used for:\n",
    "- **Sequential Data Processing** (e.g., time series, text, speech)\n",
    "- **Supervised** learning\n",
    "- **Applications**: Language Modeling, Machine Translation, Speech Recognition, Time Series Forecasting, and more.\n",
    "\n",
    "### How LSTMs Work\n",
    "- **Memory cell:**\n",
    "  - LSTMs introduce a memory cell that maintains information over long time periods, enabling the network to learn long-term dependencies.\n",
    "- **Gates:**\n",
    "  - LSTMs use three gates to regulate the flow of information:\n",
    "    - **Forget Gate:** Decides what information to discard from the cell state.\n",
    "    - **Input Gate:** Decides what new information to add to the cell state.\n",
    "    - **Output Gate:** Decides what part of the cell state to output as the hidden state.\n",
    "- **Cell state:**\n",
    "  - The cell state is the key component of LSTMs, allowing information to flow through the network with minimal modifications, mitigating the vanishing gradient problem.\n",
    "- **Sequential processing:**\n",
    "  - Like RNNs, LSTMs process sequences one element at a time, maintaining a hidden state that is updated at each time step.\n",
    "- **Backpropagation Through Time (BPTT):**\n",
    "  - LSTMs are trained using BPTT, which computes gradients through the entire sequence and updates the network weights accordingly.\n",
    "- **Long-term dependencies:**\n",
    "  - LSTMs are designed to remember information for long periods, making them suitable for tasks where long-term context is important.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`hidden_size`:**\n",
    "  - **Description:** Number of units in the LSTM’s hidden layer.\n",
    "  - **Impact:** Larger hidden sizes allow the model to capture more complex patterns but increase computational cost and risk of overfitting.\n",
    "  - **Default:** Varies; typically ranges from `128` to `512`.\n",
    "- **`num_layers`:**\n",
    "  - **Description:** Number of stacked LSTM layers.\n",
    "  - **Impact:** More layers can capture deeper temporal dependencies but may require more regularization to prevent overfitting.\n",
    "  - **Default:** `1` (can be increased for deeper models).\n",
    "- **`learning_rate`:**\n",
    "  - **Description:** Step size for updating weights during training.\n",
    "  - **Impact:** Higher values speed up training but may cause instability; lower values provide more stable convergence but slow down training.\n",
    "  - **Default:** `0.001` (varies with optimizer).\n",
    "- **`dropout_rate`:**\n",
    "  - **Description:** Fraction of units to drop during training to prevent overfitting.\n",
    "  - **Impact:** Helps in regularization; typical values range from `0.2` to `0.5`.\n",
    "  - **Default:** `0.0` (no dropout) but often set to `0.2-0.5` in practice.\n",
    "- **`sequence_length`:**\n",
    "  - **Description:** Length of input sequences processed by the LSTM.\n",
    "  - **Impact:** Longer sequences can capture more context but increase the risk of vanishing gradients and computational cost.\n",
    "  - **Default:** Varies depending on the problem.\n",
    "- **`batch_size`:**\n",
    "  - **Description:** Number of sequences processed in parallel during training.\n",
    "  - **Impact:** Larger batch sizes improve training stability but require more memory.\n",
    "  - **Default:** Typically `32` or `64`.\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| Capable of learning long-term dependencies in sequences | Computationally expensive, especially for long sequences or deep networks |\n",
    "| Effective for tasks involving time series, text, speech, etc. | Training can be slow due to the complexity of the network |\n",
    "| Mitigates vanishing gradient problem better than traditional RNNs | Prone to overfitting without careful regularization    |\n",
    "| Can handle inputs of varying lengths and sequences    | Requires significant hyperparameter tuning for optimal performance |\n",
    "| Well-suited for tasks with complex temporal dynamics  | Difficult to interpret learned representations         |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy (Classification):**\n",
    "  - **Description:** Ratio of correct predictions to total predictions.\n",
    "  - **Good Value:** Higher is better; values above 0.85 indicate strong model performance.\n",
    "  - **Bad Value:** Below 0.5 suggests poor model performance.\n",
    "- **Precision (Classification):**\n",
    "  - **Description:** Proportion of true positives among all positive predictions.\n",
    "  - **Good Value:** Higher values indicate fewer false positives, especially important in imbalanced datasets.\n",
    "  - **Bad Value:** Low values suggest many false positives.\n",
    "- **Recall (Classification):**\n",
    "  - **Description:** Proportion of actual positives correctly identified.\n",
    "  - **Good Value:** Higher values indicate fewer false negatives, important in recall-sensitive applications.\n",
    "  - **Bad Value:** Low values suggest many false negatives.\n",
    "- **F1 Score (Classification):**\n",
    "  - **Description:** Harmonic mean of Precision and Recall.\n",
    "  - **Good Value:** Higher values indicate a good balance between Precision and Recall.\n",
    "  - **Bad Value:** Low values suggest a poor balance between Precision and Recall.\n",
    "- **Perplexity (Language Modeling):**\n",
    "  - **Description:** Measures the uncertainty in predicting the next word in a sequence; lower perplexity indicates better performance.\n",
    "  - **Good Value:** Lower is better; values vary depending on the dataset, but the model should consistently reduce perplexity over time.\n",
    "  - **Bad Value:** High perplexity indicates poor predictive capability.\n",
    "- **Mean Squared Error (MSE) (Regression):**\n",
    "  - **Description:** Measures the average squared difference between predicted and actual values.\n",
    "  - **Good Value:** Lower is better; values close to `0` indicate high accuracy.\n",
    "  - **Bad Value:** Higher values suggest the model’s predictions deviate significantly from the actual values.\n",
    "- **AUC-ROC (Classification):**\n",
    "  - **Description:** Measures the model's ability to distinguish between classes across all thresholds.\n",
    "  - **Good Value:** Values closer to 1 indicate strong separability between classes.\n",
    "  - **Bad Value:** Values near 0.5 suggest random guessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppresses INFO and WARNING messages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load and preprocess the IMDb dataset\n",
    "max_features = 10000  # Number of words to consider as features\n",
    "maxlen = 500  # Cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 64\n",
    "\n",
    "# Load the data\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences to ensure consistent input length\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, hidden_size, input_length=maxlen))\n",
    "\n",
    "# Add LSTM layers\n",
    "for _ in range(num_layers - 1):\n",
    "    model.add(LSTM(hidden_size, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Last LSTM layer without return_sequences\n",
    "model.add(LSTM(hidden_size))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=5, \n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions = np.round(predictions).flatten()  # Convert predictions to labels\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
