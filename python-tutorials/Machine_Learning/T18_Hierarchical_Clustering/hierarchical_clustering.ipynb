{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "## Problem Type\n",
    "**Hierarchical Clustering** is primarily used for:\n",
    "- **Clustering** problems\n",
    "- **Unsupervised** learning\n",
    "\n",
    "### How Hierarchical Clustering Works\n",
    "- **Hierarchical structure:** Builds a hierarchy of clusters by either merging smaller clusters into larger ones (agglomerative) or splitting larger clusters into smaller ones (divisive).\n",
    "- **Agglomerative (Bottom-Up) Approach:**\n",
    "  - Starts with each data point as its own cluster.\n",
    "  - Iteratively merges the closest pairs of clusters until all points are in a single cluster or a stopping criterion is met.\n",
    "- **Divisive (Top-Down) Approach:**\n",
    "  - Starts with all data points in a single cluster.\n",
    "  - Iteratively splits clusters into smaller clusters until each point is its own cluster or a stopping criterion is met.\n",
    "- **Distance metrics:**\n",
    "  - Uses distance or similarity metrics like Euclidean, Manhattan, or Cosine to measure the closeness of clusters.\n",
    "- **Linkage criteria:**\n",
    "  - **Single Linkage:** Merges clusters based on the minimum distance between points in the clusters.\n",
    "  - **Complete Linkage:** Merges clusters based on the maximum distance between points in the clusters.\n",
    "  - **Average Linkage:** Merges clusters based on the average distance between points in the clusters.\n",
    "  - **Ward's Method:** Minimizes the variance within clusters when merging.\n",
    "- **Dendrogram:** A tree-like diagram that records the sequence of merges or splits, providing a visual representation of the cluster hierarchy.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`n_clusters`:**\n",
    "  - **Description:** The number of clusters to form after the hierarchical process is completed.\n",
    "  - **Impact:** Directly affects the final grouping of the data; choosing too few or too many clusters can lead to under- or over-segmentation.\n",
    "  - **Default:** Determined by the structure of the dendrogram.\n",
    "- **`linkage`:**\n",
    "  - **Description:** Determines which method to use when calculating the distance between clusters.\n",
    "  - **Impact:** Affects the hierarchy of clusters; `ward` tends to create more compact clusters, while `single` can lead to elongated clusters.\n",
    "  - **Default:** `ward`.\n",
    "- **`distance_threshold`:**\n",
    "  - **Description:** The linkage distance above which clusters will not be merged.\n",
    "  - **Impact:** Controls the depth of the dendrogram; lower values result in more clusters.\n",
    "  - **Default:** `None` (all clusters are merged).\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| Provides a visual representation (dendrogram)         | Computationally expensive, especially with large datasets |\n",
    "| Does not require a predefined number of clusters      | Sensitive to noise and outliers                        |\n",
    "| Can capture complex cluster structures                | Less effective with very large datasets                |\n",
    "| Useful for small to medium datasets                   | Choosing the right linkage and distance metric can be challenging |\n",
    "| Easily interpretable hierarchy of clusters            | Merging or splitting decisions are irreversible        |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Silhouette Score:**\n",
    "  - **Description:** Measures how similar an object is to its own cluster compared to other clusters.\n",
    "  - **Good Value:** Closer to 1 indicates well-defined clusters.\n",
    "  - **Bad Value:** Values near 0 suggest overlapping clusters, and negative values indicate incorrect clustering.\n",
    "- **Davies-Bouldin Index:**\n",
    "  - **Description:** Average similarity ratio of each cluster with the cluster most similar to it.\n",
    "  - **Good Value:** Lower values indicate better cluster separation and compactness.\n",
    "  - **Bad Value:** Higher values suggest poor cluster separation and compactness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering using AgglomerativeClustering\n",
    "hierarchical_cluster = AgglomerativeClustering(\n",
    "    n_clusters=3,  linkage=\"ward\", distance_threshold=None\n",
    ")\n",
    "labels = hierarchical_cluster.fit_predict(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_avg = silhouette_score(X_std, labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "db_index = davies_bouldin_score(X_std, labels)\n",
    "print(f\"Davies-Bouldin Index: {db_index:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality with PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', marker='o', edgecolor='k', s=100)\n",
    "plt.title('Clusters Visualized after Hierarchical Clustering')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
