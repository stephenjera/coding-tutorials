{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "## Problem Type\n",
    "**Random Forest** is primarily used for:\n",
    "- **Classification** problems\n",
    "- **Regression** problems\n",
    "- **Supervised** learning\n",
    "\n",
    "### How Random Forest Works\n",
    "- **Ensemble method:** Combines multiple decision trees (typically trained with the \"bagging\" method) to improve the model’s robustness and accuracy.\n",
    "- **Bootstrap Aggregation (Bagging):** \n",
    "  - Creates multiple subsets of the training data by random sampling with replacement.\n",
    "  - Trains each decision tree on a different subset.\n",
    "- **Random feature selection:**\n",
    "  - At each split in a tree, a random subset of features is considered.\n",
    "  - Helps in reducing the correlation between individual trees.\n",
    "- **Aggregation of results:**\n",
    "  - **Classification:** Takes a majority vote from all trees to make the final prediction.\n",
    "  - **Regression:** Averages the predictions of all trees.\n",
    "- **Reduces overfitting:** Due to the averaging of multiple trees, Random Forest tends to generalize better than a single decision tree.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`n_estimators`:**\n",
    "  - **Description:** Number of trees in the forest.\n",
    "  - **Impact:** More trees generally improve performance but increase computational cost.\n",
    "  - **Default:** `100` trees.\n",
    "- **`max_depth`:**\n",
    "  - **Description:** Maximum depth of the trees.\n",
    "  - **Impact:** Deeper trees can capture more detail but risk overfitting; shallower trees are less prone to overfitting.\n",
    "  - **Default:** `None` (expand until all leaves are pure or until `min_samples_split` is reached).\n",
    "- **`min_samples_split`:**\n",
    "  - **Description:** Minimum number of samples required to split an internal node.\n",
    "  - **Impact:** Higher values prevent the model from learning overly specific patterns, reducing overfitting.\n",
    "  - **Default:** `2`.\n",
    "- **`min_samples_leaf`:**\n",
    "  - **Description:** Minimum number of samples required to be at a leaf node.\n",
    "  - **Impact:** Larger values create smoother models and help prevent overfitting.\n",
    "  - **Default:** `1`.\n",
    "- **`max_features`:**\n",
    "  - **Description:** Number of features to consider when looking for the best split.\n",
    "  - **Impact:** Smaller subsets of features reduce correlation between trees, improving generalization; `sqrt(n_features)` is a common choice.\n",
    "  - **Default:** `auto` (equivalent to `sqrt(n_features)` for classification).\n",
    "- **`bootstrap`:**\n",
    "  - **Description:** Whether bootstrap samples are used when building trees.\n",
    "  - **Impact:** `True` enables bagging, which is crucial for Random Forest’s performance.\n",
    "  - **Default:** `True`.\n",
    "- **`oob_score`:**\n",
    "  - **Description:** Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
    "  - **Impact:** Provides an internal cross-validation score without needing a separate validation set.\n",
    "  - **Default:** `False`.\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| Reduces overfitting by averaging multiple trees       | Computationally expensive, especially with many trees  |\n",
    "| Can handle both numerical and categorical data        | Large model size can be difficult to interpret         |\n",
    "| Robust to outliers and noise                          | Can still overfit if trees are too deep and `n_estimators` is too low |\n",
    "| Automatically handles missing data                    | Requires more memory and storage than single decision trees |\n",
    "| Good performance with default parameters              | Slower to predict compared to simpler models           |\n",
    "| Provides feature importance ranking                   | Difficult to implement in real-time systems due to complexity |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy (Classification):**\n",
    "  - **Description:** Ratio of correct predictions to total predictions.\n",
    "  - **Good Value:** Higher is better; generally, above 0.85 is considered good.\n",
    "  - **Bad Value:** Below 0.5 suggests poor model performance.\n",
    "- **Precision (Classification):**\n",
    "  - **Description:** Proportion of positive identifications that were actually correct.\n",
    "  - **Good Value:** Higher values indicate fewer false positives, especially important in imbalanced datasets.\n",
    "  - **Bad Value:** Low values suggest many false positives.\n",
    "- **Recall (Classification):**\n",
    "  - **Description:** Proportion of actual positives that were correctly identified.\n",
    "  - **Good Value:** Higher values indicate fewer false negatives, crucial when missing positive cases is costly.\n",
    "  - **Bad Value:** Low values suggest many false negatives.\n",
    "- **F1 Score (Classification):**\n",
    "  - **Description:** Harmonic mean of Precision and Recall.\n",
    "  - **Good Value:** Higher values indicate a good balance between Precision and Recall.\n",
    "  - **Bad Value:** Low values suggest an imbalance, with either high false positives or false negatives.\n",
    "- **Mean Squared Error (MSE) (Regression):**\n",
    "  - **Description:** Average of the squared differences between predicted and actual values.\n",
    "  - **Good Value:** Lower values are better, indicating a closer fit to the data.\n",
    "  - **Bad Value:** Higher values indicate poor prediction accuracy.\n",
    "- **R-squared (R²) (Regression):**\n",
    "  - **Description:** Proportion of variance in the dependent variable predictable from the independent variables.\n",
    "  - **Good Value:** Closer to 1 is better, indicating a strong explanatory power.\n",
    "  - **Bad Value:** Values close to 0 suggest the model explains little of the variance.\n",
    "- **Feature Importance:**\n",
    "  - **Description:** Measures the importance of each feature in the prediction process.\n",
    "  - **Good Value:** High values for relevant features indicate the model relies on significant predictors.\n",
    "  - **Bad Value:** High values for irrelevant features suggest the model might be overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine dataset\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "# Separate features and target variable\n",
    "X = wine.data  # Features\n",
    "y = wine.target  # Target labels (wine type)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a Random Forest Classifier\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=10,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=wine.target_names)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "\n",
    "# Print feature importances (optional)\n",
    "for feature, importance in zip(wine.feature_names, importances):\n",
    "    print(f\"{feature}: {importance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plot_tree(model.estimators_[0], \n",
    "               filled=True, \n",
    "               rounded = True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
