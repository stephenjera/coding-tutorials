{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization (Ridge and Lasso)\n",
    "\n",
    "## Problem Type\n",
    "**Regularization** is primarily used for:\n",
    "- **Regression** and **Classification** problems\n",
    "- **Supervised** learning\n",
    "\n",
    "### How Regularization Works\n",
    "- **Introduces a penalty term** to the loss function to discourage overly complex models, which helps prevent overfitting.\n",
    "- **Ridge Regularization (L2):**\n",
    "  - Adds the squared magnitude of coefficients as a penalty term.\n",
    "  - Shrinks coefficients but does not set any to zero, retaining all features.\n",
    "- **Lasso Regularization (L1):**\n",
    "  - Adds the absolute value of coefficients as a penalty term.\n",
    "  - Can shrink some coefficients to zero, effectively performing feature selection.\n",
    "- **Elastic Net:**\n",
    "  - Combines both L1 and L2 penalties, balancing the strengths of Ridge and Lasso.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`alpha`:**\n",
    "  - **Description:** Controls the regularization strength for both Lasso and Ridge (`alpha = λ` in scikit-learn).\n",
    "  - **Impact:** Larger values of `alpha` increase the regularization, which reduces overfitting but can also lead to underfitting if too high.\n",
    "  - **Default:** `alpha = 1.0` is the default value.\n",
    "- **`l1_ratio`:**\n",
    "  - **Description:** Used in Elastic Net to mix L1 and L2 penalties. `l1_ratio = 0` is equivalent to Ridge, while `l1_ratio = 1` is equivalent to Lasso.\n",
    "  - **Impact:** Determines the balance between L1 and L2 regularization in Elastic Net.\n",
    "- **`fit_intercept`:**\n",
    "  - **Description:** Whether to calculate the intercept for this model.\n",
    "  - **Impact:** Setting to `False` forces the model to pass through the origin.\n",
    "  - **Default:** `True`, calculating the intercept.\n",
    "- **`max_iter`:**\n",
    "  - **Description:** Maximum number of iterations for the solver to converge.\n",
    "  - **Impact:** Increase if the model does not converge.\n",
    "  - **Default:** `1000` for most regularization techniques.\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| Helps prevent overfitting by penalizing large coefficients | Can lead to underfitting if regularization is too strong |\n",
    "| Lasso can perform automatic feature selection          | Ridge does not reduce coefficients to zero              |\n",
    "| Elastic Net balances the benefits of Lasso and Ridge   | Lasso may struggle with correlated features             |\n",
    "| Improves generalization of the model                   | Regularization adds complexity to model tuning          |\n",
    "| Useful when dealing with multicollinearity             | Requires careful selection of regularization strength   |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Mean Squared Error (MSE):**\n",
    "  - **Description:** Average of squared errors between predicted and actual values.\n",
    "  - **Good Value:** Lower values indicate fewer errors, with the model appropriately balanced between bias and variance.\n",
    "  - **Bad Value:** Higher values suggest poor model performance, possibly due to too much regularization.\n",
    "- **R-squared (R²):**\n",
    "  - **Description:** Proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "  - **Good Value:** Closer to 1 indicates a good fit, but values should be carefully interpreted in regularized models.\n",
    "  - **Bad Value:** Closer to 0 suggests the model does not explain much of the variance.\n",
    "- **Cross-Validation Score:**\n",
    "  - **Description:** Average performance across multiple subsets of the dataset, useful for assessing model generalization.\n",
    "  - **Good Value:** Higher scores indicate better generalization.\n",
    "  - **Bad Value:** Low or highly variable scores across folds suggest poor generalization or model instability.\n",
    "- **Coefficient Magnitudes:**\n",
    "  - **Description:** Size of the model coefficients; regularization should ideally reduce these without sacrificing accuracy.\n",
    "  - **Good Value:** Smaller, more uniform coefficients indicate effective regularization.\n",
    "  - **Bad Value:** Large or highly varying coefficients may suggest insufficient regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'target', 'target_names']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = housing.data, housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (L2 Regularization): 0.56\n",
      "R2 score (L2 Regularization): 0.5758549611440139\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ridge regression model with L2 regularization\n",
    "# You can adjust the alpha (lambda) value\n",
    "ridge_model = Ridge(\n",
    "    alpha=1.0, fit_intercept=True, max_iter=1000, solver=\"auto\", random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (L2 Regularization): {mse:.2f}\")\n",
    "print(f'R2 score (L2 Regularization): {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (L1 Regularization): 0.94\n",
      "R2 score (L1 Regularization): 0.2841671821008396\n"
     ]
    }
   ],
   "source": [
    "# Initialize Lasso regression model with L1 regularization\n",
    "# You can adjust the alpha (lambda) value\n",
    "lasso_model = Lasso(\n",
    "    alpha=1.0, fit_intercept=True, max_iter=1000,random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) and R2 score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (L1 Regularization): {mse:.2f}\")\n",
    "print(f'R2 score (L1 Regularization): {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (ElasticNet): 0.76\n",
      "R2 score (ElasticNet): 0.41655189098028234\n"
     ]
    }
   ],
   "source": [
    "# Initialize ElasticNet regression model with both L1 and L2 regularization\n",
    "# You can adjust the alpha (lambda) value and the l1_ratio\n",
    "elasticnet_model = ElasticNet(\n",
    "    alpha=1.0, l1_ratio=0.5, fit_intercept=True, max_iter=1000, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "elasticnet_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = elasticnet_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) and R2 score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (ElasticNet): {mse:.2f}\")\n",
    "print(f'R2 score (ElasticNet): {r2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
