{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "## Problem Type\n",
    "**Naive Bayes** is primarily used for:\n",
    "- **Classification** problems\n",
    "- **Supervised** learning\n",
    "\n",
    "### How Naive Bayes Works\n",
    "- **Bayes' Theorem:**\n",
    "  - Uses Bayes' theorem to calculate the probability of a class given the feature values.\n",
    "  - Assumes independence between features, meaning the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "- **Types of Naive Bayes Classifiers:**\n",
    "  - **Gaussian Naive Bayes:** Assumes that features follow a Gaussian distribution (used for continuous data).\n",
    "  - **Multinomial Naive Bayes:** Typically used for text classification, assumes feature vectors represent frequencies or counts (used for discrete data).\n",
    "  - **Bernoulli Naive Bayes:** Used when features are binary (e.g., the presence or absence of a word in text classification).\n",
    "- **Decision rule:**\n",
    "  - Classifies instances by selecting the class with the highest posterior probability given the feature values.\n",
    "- **Training process:**\n",
    "  - Involves calculating the prior probabilities of each class and the likelihood of each feature given the class.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`var_smoothing`:**\n",
    "  - **Description:** Portion of the largest variance of all features added to variances for stability.\n",
    "  - **Impact:** Prevents division by zero errors and handles numerical stability in Gaussian Naive Bayes.\n",
    "  - **Default:** `1e-9`.\n",
    "- **`alpha` (for Multinomial and Bernoulli Naive Bayes):**\n",
    "  - **Description:** Smoothing parameter to handle zero probabilities (Laplace smoothing).\n",
    "  - **Impact:** Helps in handling features not present in the training set, especially in text classification.\n",
    "  - **Default:** `1.0`.\n",
    "- **`fit_prior`:**\n",
    "  - **Description:** Whether to learn class prior probabilities or use uniform priors.\n",
    "  - **Impact:** Affects the bias towards certain classes, useful in imbalanced datasets.\n",
    "  - **Default:** `True`.\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| Simple and fast to train                               | Strong assumption of feature independence, which is rarely true in real data |\n",
    "| Works well with small datasets                        | Less flexible and can perform poorly with correlated features |\n",
    "| Performs well with high-dimensional data              | May struggle with very small sample sizes or noisy data |\n",
    "| Effective for text classification and spam filtering  | Assumes normally distributed features in Gaussian Naive Bayes, which may not hold |\n",
    "| Robust to irrelevant features                         | Smoothing parameters like `alpha` require careful tuning in some cases |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy (Classification):**\n",
    "  - **Description:** Ratio of correct predictions to total predictions.\n",
    "  - **Good Value:** Higher is better; values above 0.85 indicate good performance.\n",
    "  - **Bad Value:** Below 0.5 suggests poor model performance.\n",
    "- **Precision (Classification):**\n",
    "  - **Description:** Proportion of positive identifications that were actually correct.\n",
    "  - **Good Value:** Higher values indicate fewer false positives; crucial in imbalanced datasets.\n",
    "  - **Bad Value:** Low values suggest many false positives.\n",
    "- **Recall (Classification):**\n",
    "  - **Description:** Proportion of actual positives that were correctly identified.\n",
    "  - **Good Value:** Higher values indicate fewer false negatives; important in recall-sensitive applications.\n",
    "  - **Bad Value:** Low values suggest many false negatives.\n",
    "- **F1 Score (Classification):**\n",
    "  - **Description:** Harmonic mean of Precision and Recall.\n",
    "  - **Good Value:** Higher values indicate a good balance between Precision and Recall.\n",
    "  - **Bad Value:** Low values suggest an imbalance, with either high false positives or false negatives.\n",
    "- **AUC-ROC (Classification):**\n",
    "  - **Description:** Measures the ability of the model to distinguish between classes across all thresholds.\n",
    "  - **Good Value:** Values closer to 1 indicate strong separability between classes.\n",
    "  - **Bad Value:** Values near 0.5 suggest random guessing.\n",
    "- **Log Loss (Classification):**\n",
    "  - **Description:** Measures the performance of a classification model where the output is a probability value between 0 and 1.\n",
    "  - **Good Value:** Lower values indicate better model calibration and performance.\n",
    "  - **Bad Value:** Higher values suggest poor probabilistic predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Convert to a DataFrame for easier exploration\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "df['species'] = df['target'].apply(lambda x: iris.target_names[x])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) and target (y)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Display the shapes of the splits\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gaussian Naive Bayes model\n",
    "gnb = GaussianNB(var_smoothing = 1e-9)\n",
    "\n",
    "# Train the model\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
