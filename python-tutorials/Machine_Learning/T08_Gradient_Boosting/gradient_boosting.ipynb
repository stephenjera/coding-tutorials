{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost (Gradient Boosting)\n",
    "\n",
    "## Problem Type\n",
    "**XGBoost (eXtreme Gradient Boosting)** is primarily used for:\n",
    "- **Classification** problems\n",
    "- **Regression** problems\n",
    "- **Supervised** learning\n",
    "\n",
    "### How XGBoost Works\n",
    "- **Boosting technique:**\n",
    "  - Sequentially builds models that correct the errors of previous models.\n",
    "  - Each new model focuses more on the misclassified examples from the previous model.\n",
    "- **Gradient boosting:**\n",
    "  - Models are trained using gradient descent to minimize a loss function, making the approach effective for both regression and classification tasks.\n",
    "- **Weighted ensemble:**\n",
    "  - The final prediction is a weighted sum of all individual model predictions.\n",
    "  - Reduces bias and variance, leading to better generalization.\n",
    "- **Regularization:**\n",
    "  - Includes both L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting.\n",
    "- **Efficient computation:**\n",
    "  - Utilizes advanced techniques like tree pruning, parallel processing, and out-of-core computing to optimize both speed and performance.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`n_estimators`:**\n",
    "  - **Description:** Number of boosting rounds (trees) to be built.\n",
    "  - **Impact:** More trees generally improve performance but can lead to overfitting if too high.\n",
    "  - **Default:** `100`.\n",
    "- **`learning_rate`:**\n",
    "  - **Description:** Shrinks the contribution of each tree by the learning rate.\n",
    "  - **Impact:** Smaller values require more trees but provide better generalization; typically in the range of `0.01` to `0.3`.\n",
    "  - **Default:** `0.1`.\n",
    "- **`max_depth`:**\n",
    "  - **Description:** Maximum depth of a tree.\n",
    "  - **Impact:** Deeper trees can capture more information but may lead to overfitting; often set between `3` and `10`.\n",
    "  - **Default:** `6`.\n",
    "- **`min_child_weight`:**\n",
    "  - **Description:** Minimum sum of instance weight (hessian) needed in a child.\n",
    "  - **Impact:** Higher values prevent overfitting by making the algorithm more conservative.\n",
    "  - **Default:** `1`.\n",
    "- **`subsample`:**\n",
    "  - **Description:** Proportion of training data used to grow each tree.\n",
    "  - **Impact:** Values less than `1.0` prevent overfitting; typically in the range of `0.5` to `1.0`.\n",
    "  - **Default:** `1.0`.\n",
    "- **`colsample_bytree`:**\n",
    "  - **Description:** Fraction of features used for each tree.\n",
    "  - **Impact:** Reducing this can help prevent overfitting and speed up training.\n",
    "  - **Default:** `1.0`.\n",
    "- **`gamma`:**\n",
    "  - **Description:** Minimum loss reduction required to make a further partition on a leaf node.\n",
    "  - **Impact:** Higher values make the algorithm more conservative by preventing unnecessary splits.\n",
    "  - **Default:** `0`.\n",
    "- **`reg_alpha` and `reg_lambda`:**\n",
    "  - **Description:** L1 and L2 regularization terms, respectively.\n",
    "  - **Impact:** Higher values add more penalty, reducing overfitting.\n",
    "  - **Default:** `0` (no regularization).\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| High predictive accuracy                              | Computationally expensive; requires significant resources |\n",
    "| Built-in regularization to prevent overfitting        | Complex model that can be difficult to interpret       |\n",
    "| Handles missing data internally                       | Sensitive to noisy data                                |\n",
    "| Supports parallel processing for faster training      | Requires careful tuning of many hyperparameters        |\n",
    "| Scales well to large datasets                         | Tendency to overfit if `n_estimators` or `max_depth` are too high |\n",
    "| Works well with imbalanced datasets                   | May require significant trial and error in tuning      |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy (Classification):**\n",
    "  - **Description:** Ratio of correct predictions to total predictions.\n",
    "  - **Good Value:** Higher is better; values above 0.85 typically indicate good performance.\n",
    "  - **Bad Value:** Below 0.5 suggests poor model performance.\n",
    "- **Precision (Classification):**\n",
    "  - **Description:** Proportion of positive identifications that were actually correct.\n",
    "  - **Good Value:** Higher values indicate fewer false positives; important in cases with imbalanced classes.\n",
    "  - **Bad Value:** Low values suggest many false positives.\n",
    "- **Recall (Classification):**\n",
    "  - **Description:** Proportion of actual positives that were correctly identified.\n",
    "  - **Good Value:** Higher values indicate fewer false negatives; crucial in recall-sensitive applications.\n",
    "  - **Bad Value:** Low values suggest many false negatives.\n",
    "- **F1 Score (Classification):**\n",
    "  - **Description:** Harmonic mean of Precision and Recall.\n",
    "  - **Good Value:** Higher values indicate a good balance between Precision and Recall.\n",
    "  - **Bad Value:** Low values suggest a poor balance, with either high false positives or false negatives.\n",
    "- **Mean Squared Error (MSE) (Regression):**\n",
    "  - **Description:** Average of the squared differences between predicted and actual values.\n",
    "  - **Good Value:** Lower values indicate better model performance; values close to 0 are ideal.\n",
    "  - **Bad Value:** High values suggest poor predictive accuracy.\n",
    "- **R-squared (RÂ²) (Regression):**\n",
    "  - **Description:** Proportion of variance in the dependent variable predictable from the independent variables.\n",
    "  - **Good Value:** Closer to 1 indicates a strong model fit.\n",
    "  - **Bad Value:** Values near 0 suggest the model does not explain much of the variance.\n",
    "- **Log Loss (Classification):**\n",
    "  - **Description:** Measures the performance of a classification model where the output is a probability value between 0 and 1.\n",
    "  - **Good Value:** Lower values are better; close to 0 indicates strong performance.\n",
    "  - **Bad Value:** Higher values indicate poor model calibration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine dataset\n",
    "wine = load_wine()\n",
    "\n",
    "# Separate features and target variable\n",
    "X = wine.data  # Features\n",
    "y = wine.target  # Target labels (wine type)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XGBoost parameters with your specified values\n",
    "params = {\n",
    "    \"objective\": \"multi:softmax\",  # Multi-class classification\n",
    "    \"num_class\": 3,  # Number of classes\n",
    "    \"n_estimators\": 1,  # Number of trees (boosting rounds)\n",
    "    \"learning_rate\": 0.1,  # Learning rate\n",
    "    \"max_depth\": 5,  # Maximum depth of each tree\n",
    "    \"min_child_weight\": 3,  # Minimum sum of instance weight (hessian) in a child\n",
    "    \"subsample\": 0.8,  # Proportion of training data used for each tree\n",
    "    \"colsample_bytree\": 0.8,  # Fraction of features used for each tree\n",
    "    \"gamma\": 0.1,  # Minimum loss reduction to make a further partition on a leaf node\n",
    "    \"reg_alpha\": 10,  # L1 regularization term\n",
    "    \"reg_lambda\": 1,  # L2 regularization term\n",
    "    \"seed\": 42,  # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier with the specified parameters\n",
    "model = xgb.XGBClassifier(**params)\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Get the predicted class probabilities\n",
    "y_pred_proba = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=wine.target_names)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f\"Log Loss: {log_loss(y_test, y_pred_proba)}\")\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = model.get_booster()\n",
    "\n",
    "# Get the number of trees\n",
    "# num_trees = model.n_estimators\n",
    "\n",
    "# Alternative way to get the total number of trees directly from the booster\n",
    "num_trees_boost = booster.trees_to_dataframe()['Tree'].max() + 1\n",
    "\n",
    "print(f\"Total number of trees: {num_trees_boost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all trees\n",
    "for i in range(num_trees_boost):\n",
    "    xgb.plot_tree(model, num_trees=i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
