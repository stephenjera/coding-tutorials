{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "## Problem Type\n",
    "**Support Vector Machines (SVM)** are primarily used for:\n",
    "- **Classification** (binary and multiclass)\n",
    "- **Regression** (Support Vector Regression - SVR)\n",
    "- **Supervised** learning\n",
    "\n",
    "### How SVM Works\n",
    "- **Finds the optimal hyperplane** that maximizes the margin between the closest data points of different classes (support vectors).\n",
    "- **Maximizes the margin** between classes to ensure better generalization.\n",
    "- **Uses different kernel functions** (linear, polynomial, RBF, etc.) to handle non-linearly separable data by transforming the data into a higher-dimensional space.\n",
    "- **Supports both hard margin and soft margin** classification:\n",
    "  - **Hard margin**: Requires all data points to be correctly classified.\n",
    "  - **Soft margin**: Allows some misclassification to create a more generalizable model.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`C`:**\n",
    "  - **Description:** Regularization parameter that controls the trade-off between maximizing the margin and minimizing classification errors.\n",
    "  - **Impact:** Smaller `C` values increase the margin but allow more misclassifications (higher bias, lower variance); larger `C` values make the margin smaller with fewer misclassifications (lower bias, higher variance).\n",
    "  - **Default:** `C = 1.0`.\n",
    "- **`kernel`:**\n",
    "  - **Description:** Specifies the kernel type to be used in the algorithm (`linear`, `poly`, `rbf`, `sigmoid`).\n",
    "  - **Impact:** Determines the decision boundary shape. The `linear` kernel is best for linearly separable data; `rbf` and `poly` can handle non-linear relationships.\n",
    "  - **Default:** `rbf` (Radial Basis Function).\n",
    "- **`gamma`:**\n",
    "  - **Description:** Kernel coefficient for `rbf`, `poly`, and `sigmoid` kernels.\n",
    "  - **Impact:** Controls the influence of individual training examples. Lower `gamma` values mean a larger influence radius (smoother decision boundary); higher `gamma` values lead to a more complex boundary.\n",
    "  - **Default:** `scale`, which is `1 / (n_features * X.var())`.\n",
    "- **`degree`:**\n",
    "  - **Description:** Degree of the polynomial kernel function (only used with `poly` kernel).\n",
    "  - **Impact:** Determines the flexibility of the decision boundary when using polynomial kernels.\n",
    "  - **Default:** `degree = 3`.\n",
    "- **`coef0`:**\n",
    "  - **Description:** Independent term in kernel function (used for `poly` and `sigmoid`).\n",
    "  - **Impact:** Controls the influence of higher-degree polynomials in the decision function.\n",
    "  - **Default:** `coef0 = 0.0`.\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| Effective for high-dimensional spaces                 | Computationally intensive, especially with large datasets |\n",
    "| Works well with clear margin of separation            | Choosing the right kernel and hyperparameters can be complex |\n",
    "| Handles both linear and non-linear classification     | Sensitive to noisy data and outliers                   |\n",
    "| Flexible through different kernel choices             | SVM models can be difficult to interpret               |\n",
    "| Supports soft margin for better generalization        | Memory-intensive during training                       |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy:**\n",
    "  - **Description:** Ratio of correct predictions to total predictions.\n",
    "  - **Good Value:** Higher is better; generally, above 0.8 is considered good for classification tasks.\n",
    "  - **Bad Value:** Below 0.5 indicates poor performance (worse than random guessing in binary classification).\n",
    "- **Precision:**\n",
    "  - **Description:** Proportion of positive identifications that were actually correct (True Positives / (True Positives + False Positives)).\n",
    "  - **Good Value:** Higher is better, especially when False Positives are costly.\n",
    "  - **Bad Value:** Low values indicate many False Positives.\n",
    "- **Recall (Sensitivity):**\n",
    "  - **Description:** Proportion of actual positives that were correctly identified (True Positives / (True Positives + False Negatives)).\n",
    "  - **Good Value:** Higher is better, especially when False Negatives are costly.\n",
    "  - **Bad Value:** Low values indicate many False Negatives.\n",
    "- **F1 Score:**\n",
    "  - **Description:** Harmonic mean of Precision and Recall; balances the trade-off between the two.\n",
    "  - **Good Value:** Higher is better; values above 0.7-0.8 indicate strong performance.\n",
    "  - **Bad Value:** Lower values indicate an imbalanced trade-off between Precision and Recall.\n",
    "- **ROC-AUC:**\n",
    "  - **Description:** Area under the Receiver Operating Characteristic curve, showing the trade-off between True Positive Rate (TPR) and False Positive Rate (FPR).\n",
    "  - **Good Value:** Closer to 1 is better; above 0.8 indicates good discrimination between classes.\n",
    "  - **Bad Value:** Close to 0.5 suggests the model is no better than random guessing.\n",
    "- **Support Vectors:**\n",
    "  - **Description:** Number of support vectors used by the model; a higher number can indicate a more complex model.\n",
    "  - **Good Value:** Lower numbers can indicate a simpler, more generalizable model.\n",
    "  - **Bad Value:** Very high numbers can indicate overfitting or a model struggling with noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Support Vector Classifier (SVC)\n",
    "svc = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = svc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Predict class labels\n",
    "y_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'ROC-AUC Score: {roc_auc:.2f}')\n",
    "\n",
    "# Plot the ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report and accuracy\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data[:, :2]  # Use only the first two features for visualization\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Support Vector Classifier (SVC)\n",
    "svc = SVC(\n",
    "    C=1,\n",
    "    kernel=\"linear\",\n",
    "    gamma='scale',\n",
    "    probability=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = svc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Predict class labels\n",
    "y_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary and support vectors\n",
    "def plot_svc_decision_boundary(model, X, y):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.bwr, s=30)\n",
    "\n",
    "    # Plot decision boundary\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # Create grid to evaluate model\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100)\n",
    "    )\n",
    "\n",
    "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot decision boundary and margins\n",
    "    plt.contour(\n",
    "        xx,\n",
    "        yy,\n",
    "        Z,\n",
    "        colors=\"k\",\n",
    "        levels=[-1, 0, 1],\n",
    "        alpha=0.5,\n",
    "        linestyles=[\"--\", \"-\", \"--\"],\n",
    "    )\n",
    "\n",
    "    # Plot support vectors\n",
    "    plt.scatter(\n",
    "        model.support_vectors_[:, 0],\n",
    "        model.support_vectors_[:, 1],\n",
    "        s=100,\n",
    "        linewidth=1,\n",
    "        facecolors=\"none\",\n",
    "        edgecolors=\"k\",\n",
    "    )\n",
    "    plt.xlabel(data.feature_names[0])\n",
    "    plt.ylabel(data.feature_names[1])\n",
    "    plt.title(\"SVM Decision Boundary with Support Vectors\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_svc_decision_boundary(svc, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
