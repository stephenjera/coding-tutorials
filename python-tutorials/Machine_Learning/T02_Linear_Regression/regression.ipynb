{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Problem Type\n",
    "**Linear Regression** is primarily used for:\n",
    "- **Regression** problems\n",
    "- **Supervised** learning\n",
    "\n",
    "### How Linear Regression Works\n",
    "- **Assumes a linear relationship** between the input variables (features) and the output variable (target).\n",
    "- **Fits a line** (in simple linear regression) or a hyperplane (in multiple linear regression) to minimize the difference between actual and predicted values.\n",
    "- **Uses the Ordinary Least Squares (OLS) method** to minimize the sum of squared residuals (differences between observed and predicted values).\n",
    "- **Calculates coefficients (weights)** for each feature to determine the best-fit line/hyperplane.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`fit_intercept`:** \n",
    "  - Controls whether to calculate the intercept (`True` by default).\n",
    "  - Setting to `False` forces the line to go through the origin (0,0).\n",
    "- **`n_jobs`:**\n",
    "  - Specifies the number of CPUs to use for computation.\n",
    "  - `-1` uses all processors, speeding up the computation on large datasets.\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                | Cons                                               |\n",
    "|-----------------------------------------------------|----------------------------------------------------|\n",
    "| Simple to understand and implement                  | Assumes a linear relationship between variables    |\n",
    "| Interpretable coefficients                          | Sensitive to outliers                              |\n",
    "| Computationally efficient for small to medium datasets | Limited to linear relationships                    |\n",
    "| Works well when the relationship is approximately linear | Prone to multicollinearity if features are highly correlated |\n",
    "| Provides insights into the relative importance of features | Can be overfitted if not properly regularized      |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Mean Absolute Error (MAE):**\n",
    "  - **Description:** Average of absolute errors between predicted and actual values.\n",
    "  - **Good Value:** Lower values indicate better model performance.\n",
    "  - **Bad Value:** Higher values suggest poor model accuracy.\n",
    "- **Mean Squared Error (MSE):**\n",
    "  - **Description:** Average of squared errors between predicted and actual values.\n",
    "  - **Good Value:** Lower values indicate fewer errors.\n",
    "  - **Bad Value:** Higher values indicate greater errors; sensitive to outliers.\n",
    "- **Root Mean Squared Error (RMSE):**\n",
    "  - **Description:** Square root of the mean squared errors; gives error in the same units as the target variable.\n",
    "  - **Good Value:** Lower values indicate better fit.\n",
    "  - **Bad Value:** Higher values indicate poor model performance.\n",
    "- **R-squared (R²):**\n",
    "  - **Description:** Proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "  - **Good Value:** Closer to 1 (e.g., 0.9+) suggests a good fit.\n",
    "  - **Bad Value:** Closer to 0 (e.g., 0.5 or lower) suggests a poor fit.\n",
    "- **Adjusted R-squared:**\n",
    "  - **Description:** R² adjusted for the number of predictors in the model; penalizes adding irrelevant features.\n",
    "  - **Good Value:** Higher values are better, but should also be close to R².\n",
    "  - **Bad Value:** A large drop from R² suggests overfitting with unnecessary features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df[\"MedHouseValue\"] = housing.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.loc[df[\"MedHouseValue\"] == max(df[\"MedHouseValue\"])].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"MedHouseValue\", axis=1)\n",
    "y = df[\"MedHouseValue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (optional but recommended)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression(fit_intercept=True, n_jobs=-1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training score: {linear_model.score(X_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = X.columns\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(linear_model.coef_, predictors).sort_values()\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linear_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_actual = pd.DataFrame({\"predicted\": y_pred, \"actual\": y_test})\n",
    "df_pred_actual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"y_test\")\n",
    "plt.ylabel(\"y_pred\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_actual_sample = df_pred_actual.sample(100)\n",
    "df_pred_actual_sample = df_pred_actual_sample.reset_index()\n",
    "df_pred_actual_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(df_pred_actual_sample[\"predicted\"], label=\"predicted\")\n",
    "plt.plot(df_pred_actual_sample[\"actual\"], label=\"actual\")\n",
    "\n",
    "plt.ylabel(\"median_house_value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error \n",
    "- **Interpretation:** Measures how far off the predictions are from the correct values on average, in squared units of the target variable. Lower MSE indicates better model performance (closer predictions to actual values).\n",
    "- **Good vs. Bad Values:** There's no universal threshold, but generally, a lower MSE is better. The importance depends on the scale and range of your target variable. A small MSE on a dataset with values in the range of 0-1 might be less significant than a similar MSE on a dataset with values in the 1000-2000 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error (RMSE):\n",
    "\n",
    "- **Interpretation:** Represents the standard deviation of the errors, expressed in the same units as the target variable. It's easier to interpret than MSE because it's in the same scale as the target values.\n",
    "- **Good vs. Bad Values:** A lower RMSE is better, indicating a smaller average error magnitude. The importance depends on the scale and range of your target variable, similar to MSE and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Squared\n",
    "- **Interpretation:** A value closer to 1 indicates a better fit, meaning the model explains a higher proportion of the variance. However, R² can be misleading, especially with highly correlated features. It might increase even if the model doesn't capture the underlying relationships well.\n",
    "-  **Good vs. Bad Values:** Higher R² is generally preferred, but be cautious of overfitting. Consider it alongside other metrics for a more comprehensive evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error (MAE)\n",
    "\n",
    "- **Interpretation:** Measures the average magnitude of errors, in the same units as the target variable. It's less sensitive to outliers compared to MSE. Lower MAE suggests better model performance (smaller average prediction errors).\n",
    "- **Good vs. Bad Values:** A lower MAE is better. Similar to MSE, the significance depends on the scale and range of your target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation Scores\n",
    "\n",
    "- **Interpretation:** Provides an idea of how well the model might perform on unseen data. Scores closer to 1 for regression tasks (higher for classification) indicate better generalization ability.\n",
    "- **Good vs. Bad Values:** Higher cross-validation scores suggest better model generalizability. However, it's crucial to consider other evaluation metrics alongside this to get a more holistic understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_scores = cross_val_score(linear_model, X_train, y_train, cv=5, scoring=\"r2\")\n",
    "print(f\"Cross Validation Scores: {cross_val_scores}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
