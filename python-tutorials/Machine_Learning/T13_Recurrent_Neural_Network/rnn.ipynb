{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "## Problem Type\n",
    "**Recurrent Neural Networks (RNNs)** are primarily used for:\n",
    "- **Sequential Data Processing** (e.g., time series, text)\n",
    "- **Supervised** learning\n",
    "- **Applications**: Language Modeling, Speech Recognition, Machine Translation, Time Series Prediction, and more.\n",
    "\n",
    "### How RNNs Work\n",
    "- **Sequential processing:**\n",
    "  - RNNs process input sequences one element at a time, maintaining a hidden state that carries information about the previous elements.\n",
    "- **Hidden state:**\n",
    "  - The hidden state is updated at each time step based on the current input and the previous hidden state, allowing the model to capture temporal dependencies.\n",
    "- **Shared parameters:**\n",
    "  - The same parameters (weights) are shared across all time steps, enabling the network to generalize across different parts of the sequence.\n",
    "- **Backpropagation Through Time (BPTT):**\n",
    "  - RNNs are trained using BPTT, which extends traditional backpropagation to sequences by unrolling the network in time and computing gradients through the entire sequence.\n",
    "- **Vanishing/exploding gradients:**\n",
    "  - RNNs can suffer from vanishing or exploding gradients when dealing with long sequences, making it difficult to learn long-term dependencies.\n",
    "- **Variants:**\n",
    "  - LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) are popular variants that address the vanishing gradient problem by using gates to control the flow of information.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`hidden_size`:**\n",
    "  - **Description:** Number of units in the RNN’s hidden layer.\n",
    "  - **Impact:** Larger hidden sizes allow the model to capture more complex patterns but increase computational cost and risk of overfitting.\n",
    "  - **Default:** Varies; typically ranges from `128` to `512`.\n",
    "- **`num_layers`:**\n",
    "  - **Description:** Number of stacked RNN layers.\n",
    "  - **Impact:** More layers can capture deeper temporal dependencies but may require more regularization to prevent overfitting.\n",
    "  - **Default:** `1` (can be increased for deeper models).\n",
    "- **`learning_rate`:**\n",
    "  - **Description:** Step size for updating weights during training.\n",
    "  - **Impact:** Higher values speed up training but may cause instability; lower values provide more stable convergence but slow down training.\n",
    "  - **Default:** `0.001` (varies with optimizer).\n",
    "- **`dropout_rate`:**\n",
    "  - **Description:** Fraction of units to drop during training to prevent overfitting.\n",
    "  - **Impact:** Helps in regularization; typical values range from `0.2` to `0.5`.\n",
    "  - **Default:** `0.0` (no dropout) but often set to `0.2-0.5` in practice.\n",
    "- **`sequence_length`:**\n",
    "  - **Description:** Length of input sequences processed by the RNN.\n",
    "  - **Impact:** Longer sequences can capture more context but increase the risk of vanishing gradients and computational cost.\n",
    "  - **Default:** Varies depending on the problem.\n",
    "- **`batch_size`:**\n",
    "  - **Description:** Number of sequences processed in parallel during training.\n",
    "  - **Impact:** Larger batch sizes improve training stability but require more memory.\n",
    "  - **Default:** Typically `32` or `64`.\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                  | Cons                                                   |\n",
    "|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| Captures temporal dependencies in sequential data     | Suffers from vanishing/exploding gradients, especially with long sequences |\n",
    "| Effective for tasks involving time series, text, etc. | Computationally expensive, particularly for long sequences or deep networks |\n",
    "| Supports variable-length sequences                    | Difficult to train and tune due to complex dependencies in time |\n",
    "| LSTM and GRU variants mitigate gradient problems      | Sequential processing limits parallelization compared to CNNs |\n",
    "| Can handle inputs of varying lengths and sequences    | Prone to overfitting without careful regularization    |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy (Classification):**\n",
    "  - **Description:** Ratio of correct predictions to total predictions.\n",
    "  - **Good Value:** Higher is better; values above 0.85 indicate strong model performance.\n",
    "  - **Bad Value:** Below 0.5 suggests poor model performance.\n",
    "- **Precision (Classification):**\n",
    "  - **Description:** Proportion of true positives among all positive predictions.\n",
    "  - **Good Value:** Higher values indicate fewer false positives, especially important in imbalanced datasets.\n",
    "  - **Bad Value:** Low values suggest many false positives.\n",
    "- **Recall (Classification):**\n",
    "  - **Description:** Proportion of actual positives correctly identified.\n",
    "  - **Good Value:** Higher values indicate fewer false negatives, important in recall-sensitive applications.\n",
    "  - **Bad Value:** Low values suggest many false negatives.\n",
    "- **F1 Score (Classification):**\n",
    "  - **Description:** Harmonic mean of Precision and Recall.\n",
    "  - **Good Value:** Higher values indicate a good balance between Precision and Recall.\n",
    "  - **Bad Value:** Low values suggest a poor balance between Precision and Recall.\n",
    "- **Perplexity (Language Modeling):**\n",
    "  - **Description:** Measures the uncertainty in predicting the next word in a sequence; lower perplexity indicates better performance.\n",
    "  - **Good Value:** Lower is better; values vary depending on the dataset, but the model should consistently reduce perplexity over time.\n",
    "  - **Bad Value:** High perplexity indicates poor predictive capability.\n",
    "- **Mean Squared Error (MSE) (Regression):**\n",
    "  - **Description:** Measures the average squared difference between predicted and actual values.\n",
    "  - **Good Value:** Lower is better; values close to `0` indicate high accuracy.\n",
    "  - **Bad Value:** Higher values suggest the model’s predictions deviate significantly from the actual values.\n",
    "- **AUC-ROC (Classification):**\n",
    "  - **Description:** Measures the model's ability to distinguish between classes across all thresholds.\n",
    "  - **Good Value:** Values closer to 1 indicate strong separability between classes.\n",
    "  - **Bad Value:** Values near 0.5 suggest random guessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppresses INFO and WARNING messages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import Dense, Embedding, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "# Parameters\n",
    "max_features = 10000  # number of words to consider as features\n",
    "maxlen = 500  # cut texts after this number of words\n",
    "max_features = 10000  # Vocabulary size\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "sequence_length = 100\n",
    "batch_size = 32\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "for _ in range(num_layers - 1):\n",
    "    model.add(SimpleRNN(hidden_size, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "model.add(SimpleRNN(hidden_size))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions = np.round(predictions).flatten()  # Convert predictions to labels\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
