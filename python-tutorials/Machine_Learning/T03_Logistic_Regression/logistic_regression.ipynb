{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Problem Type\n",
    "**Logistic Regression** is primarily used for:\n",
    "- **Classification** problems (binary and multiclass)\n",
    "- **Supervised** learning\n",
    "\n",
    "### How Logistic Regression Works\n",
    "- **Models the probability** that a given input belongs to a particular class (e.g., binary classification as 0 or 1).\n",
    "- **Uses the sigmoid (logistic) function** to output probabilities between 0 and 1.\n",
    "- **Applies a threshold** (typically 0.5) to classify data into one of the categories.\n",
    "- **Maximizes the likelihood** that the observed data belongs to the predicted class via Maximum Likelihood Estimation (MLE).\n",
    "- **Extends to multiclass problems** using methods like one-vs-rest (OvR) or softmax for multinomial logistic regression.\n",
    "\n",
    "### Key Tuning Metrics\n",
    "- **`penalty`:**\n",
    "  - **Description:** Type of regularization to apply (`l1`, `l2`, `elasticnet`, or `none`).\n",
    "  - **Impact:** Regularization helps avoid overfitting by penalizing large coefficients.\n",
    "  - **Common Choices:** `l2` (Ridge) is common for Logistic Regression; `l1` (Lasso) performs feature selection.\n",
    "- **`C`:**\n",
    "  - **Description:** Inverse of regularization strength (`C = 1/Î»`).\n",
    "  - **Impact:** Smaller values increase regularization strength; larger values reduce regularization (risk of overfitting).\n",
    "  - **Default:** `C = 1.0` is the default value.\n",
    "- **`solver`:**\n",
    "  - **Description:** Optimization algorithm used in training (`liblinear`, `lbfgs`, `sag`, `saga`, etc.).\n",
    "  - **Impact:** Different solvers are better suited for specific datasets; `lbfgs` is efficient for larger datasets, while `liblinear` is good for small datasets with binary classification.\n",
    "- **`max_iter`:**\n",
    "  - **Description:** Maximum number of iterations taken by the solver to converge.\n",
    "  - **Impact:** Increase if the model does not converge (warnings about non-convergence).\n",
    "  - **Default:** 100, but may need to increase depending on dataset size.\n",
    "\n",
    "### Pros vs Cons\n",
    "\n",
    "| Pros                                                | Cons                                                   |\n",
    "|-----------------------------------------------------|--------------------------------------------------------|\n",
    "| Simple and interpretable model                      | Assumes a linear decision boundary                      |\n",
    "| Effective for binary classification                 | Can struggle with complex relationships in data         |\n",
    "| Outputs well-calibrated probabilities               | Sensitive to multicollinearity among features           |\n",
    "| Can be regularized to avoid overfitting             | Does not perform well when classes are highly imbalanced |\n",
    "| Efficient and fast to train on small to medium datasets | Assumes independence of predictors                      |\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy:**\n",
    "  - **Description:** Ratio of correct predictions to total predictions.\n",
    "  - **Good Value:** Higher is better, ideally above 0.8 for well-performing models.\n",
    "  - **Bad Value:** Lower than 0.5 indicates poor performance (worse than random guessing for binary classification).\n",
    "- **Precision:**\n",
    "  - **Description:** Proportion of positive identifications that were actually correct (True Positives / (True Positives + False Positives)).\n",
    "  - **Good Value:** Higher is better, especially when False Positives are costly (e.g., medical diagnosis).\n",
    "  - **Bad Value:** Low values indicate many False Positives.\n",
    "- **Recall (Sensitivity):**\n",
    "  - **Description:** Proportion of actual positives that were correctly identified (True Positives / (True Positives + False Negatives)).\n",
    "  - **Good Value:** Higher is better, especially when False Negatives are costly (e.g., detecting fraud).\n",
    "  - **Bad Value:** Low values indicate many False Negatives.\n",
    "- **F1 Score:**\n",
    "  - **Description:** Harmonic mean of Precision and Recall; balances the trade-off between the two.\n",
    "  - **Good Value:** Higher is better; values above 0.7-0.8 indicate strong performance.\n",
    "  - **Bad Value:** Lower values indicate imbalanced trade-off between Precision and Recall.\n",
    "- **ROC-AUC:**\n",
    "  - **Description:** Area under the Receiver Operating Characteristic curve, showing the trade-off between True Positive Rate (TPR) and False Positive Rate (FPR).\n",
    "  - **Good Value:** Closer to 1 is better; above 0.8 indicates good discrimination between classes.\n",
    "  - **Bad Value:** Close to 0.5 suggests the model is no better than random guessing.\n",
    "- **Log Loss:**\n",
    "  - **Description:** Measures the uncertainty of the model's predictions, where lower log loss is better.\n",
    "  - **Good Value:** Lower is better; values close to 0 indicate high-confidence, accurate predictions.\n",
    "  - **Bad Value:** Higher values suggest incorrect and uncertain predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, auc, confusion_matrix, f1_score,\n",
    "                             precision_score, recall_score, roc_curve, log_loss)\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that each flower only has 50 data point and is in order\n",
    "index = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data (for binary classification)\n",
    "# We'll consider only two classes: Setosa (class 0) and Versicolor (class 1)\n",
    "X_binary = X[0:index]\n",
    "y_binary = y[0:index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_binary, y_binary, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the logistic regression model\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        fit_intercept=True,\n",
    "        penalty=\"l2\",\n",
    "        C=1,\n",
    "        solver=\"liblinear\",\n",
    "        max_iter=100,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict new values\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision: {precision_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Recall: {recall_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1 Score: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Log Loss: {log_loss(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "- **Interpretation:** A confusion matrix is a table that is often used to describe the performance of a classification model. It contains information about actual and predicted classifications done by the model.\n",
    "- **Good vs. Bad Values:** In a confusion matrix, the diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. The higher the diagonal values of the confusion matrix the better, indicating many correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Confusion Matrix:\\n {confusion_matrix(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "print(f\"false_positive_rate: {false_positive_rate}\")\n",
    "print(f\"true_positive_rate: {true_positive_rate}\")\n",
    "print(f\"AUC-ROC: {auc(false_positive_rate, true_positive_rate)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X_binary, y_binary, cv=5)\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Average cross-validation score: {scores.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-ktax2Mo_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
