{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Superstore\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path =  \"../datasets\"\n",
    "orders_path = f\"{datasets_path}/global_superstore_orders_2018.csv\"\n",
    "people_path = f\"{datasets_path}/global_superstore_people_2018.csv\"\n",
    "returns_path = f\"{datasets_path}/global_superstore_returns_2018.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each CSV file into a separate DataFrame\n",
    "orders_df = spark.read.csv(orders_path,header=True,inferSchema=True)\n",
    "people_df = spark.read.csv(people_path,header=True,inferSchema=True)\n",
    "returns_df = spark.read.csv(returns_path,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrame as a table\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "people_df.createOrReplaceTempView(\"people\")\n",
    "returns_df.createOrReplaceTempView(\"returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM orders\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "--sql\n",
    "select `Ship Mode`,\n",
    "    avg(`Shipping Cost`) as average_shipping_cost\n",
    "from orders\n",
    "group by `Ship Mode`\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.groupBy('Ship Mode')\\\n",
    "    .agg(F.avg('Shipping Cost').alias('average_shipping_cost'))\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "--sql\n",
    "select \n",
    "    returns.`Returned`,\n",
    "    orders.*\n",
    "from orders\n",
    "    inner join returns on orders.`Order ID` = returns.`Order ID`\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = orders_df.join(returns_df, orders_df[\"Order ID\"] == returns_df[\"Order ID\"])\n",
    "joined_df.select(returns_df[\"Returned\"], orders_df[\"*\"]).show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env-nlv4FaUC-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
